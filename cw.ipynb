{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  python3    /vol/bitbucket/ss5120/nlp_env/share/jupyter/kernels/python3\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'ss5120'\n",
    "content_path = f'/vol/bitbucket/{USERNAME}/NLP_CW/'\n",
    "data_path = f'/vol/bitbucket/{USERNAME}/NLP_CW/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 23 12:14:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0  On |                  N/A |\n",
      "| 30%   39C    P2              38W / 320W |   3486MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2991948      C   python                                     3082MiB |\n",
      "|    0   N/A  N/A   3096920      C   /vol/bitbucket/ao420/swe/bin/python3        312MiB |\n",
      "|    0   N/A  N/A   3115365      G   /usr/lib/xorg/Xorg                           78MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (0.70.0)\n",
      "Requirement already satisfied: regex in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2023.12.25)\n",
      "Requirement already satisfied: streamlit in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (1.31.1)\n",
      "Requirement already satisfied: numpy in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (0.16.3)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (4.38.1)\n",
      "Requirement already satisfied: requests in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2.31.0)\n",
      "Requirement already satisfied: scipy in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (1.4.1.post1)\n",
      "Requirement already satisfied: tokenizers in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (0.15.2)\n",
      "Requirement already satisfied: tensorboard in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2.16.2)\n",
      "Requirement already satisfied: sentencepiece in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: seqeval in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboardx in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (4.66.2)\n",
      "Requirement already satisfied: datasets in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from simpletransformers) (2.17.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.4.2)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (3.13.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.25.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.40.5)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.42)\n",
      "Requirement already satisfied: setproctitle in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Requirement already satisfied: setuptools in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (59.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from requests->simpletransformers) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from requests->simpletransformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from requests->simpletransformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from requests->simpletransformers) (2024.2.2)\n",
      "Requirement already satisfied: multiprocess in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (15.0.0)\n",
      "Requirement already satisfied: xxhash in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from datasets->simpletransformers) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.3.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.3.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.9.0)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.0.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (10.2.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.7.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.7.0)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (7.0.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.2.0)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.22.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.4)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.62.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.5.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: six>1.9 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.21.1)\n",
      "Requirement already satisfied: toolz in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from importlib-metadata<8,>=1.4->streamlit->simpletransformers) (3.17.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.33.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: tensorboardx in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboardx) (4.25.3)\n",
      "Requirement already satisfied: numpy in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboardx) (1.26.4)\n",
      "Requirement already satisfied: packaging in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from tensorboardx) (23.2)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: torch in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: fsspec in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install tensorboardx\n",
    "!pip install sklearn\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel,\n",
    "    ClassificationArgs,\n",
    "    MultiLabelClassificationModel,\n",
    "    MultiLabelClassificationArgs,\n",
    ")\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
     ]
    }
   ],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
    "module_name = module_url.split(\"/\")[-1]\n",
    "print(f\"Fetching {module_url}\")\n",
    "# with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name, \"w\") as outf:\n",
    "    a = f.read()\n",
    "    outf.write(a.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = DontPatronizeMe('.', data_path + 'task4_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm.load_task1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trids = pd.read_csv(data_path + 'train_semeval_parids-labels.csv')\n",
    "teids = pd.read_csv(data_path + 'dev_semeval_parids-labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trids.par_id = trids.par_id.astype(str)\n",
    "teids.par_id = teids.par_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dpm.train_task1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # will contain par_id, label and text\n",
    "for idx in range(len(trids)):\n",
    "    parid = trids.par_id[idx]\n",
    "    # print(parid)\n",
    "    # select row from original dataset to retrieve `text` and binary label\n",
    "    keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "    text = data.loc[data.par_id == parid].text.values[0]\n",
    "    label = data.loc[data.par_id == parid].label.values[0]\n",
    "    rows.append(\n",
    "        {\"par_id\": parid, \"community\": keyword, \"text\": text, \"label\": label}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf1 = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136</td>\n",
       "      <td>homeless</td>\n",
       "      <td>Durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10352</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8279</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>Far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1164</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>To strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>8380</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Rescue teams search for survivors on the rubbl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>8381</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>The launch of ' Happy Birthday ' took place la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>8382</td>\n",
       "      <td>homeless</td>\n",
       "      <td>The unrest has left at least 20,000 people dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>8383</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>You have to see it from my perspective . I may...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>8384</td>\n",
       "      <td>disabled</td>\n",
       "      <td>Yet there was one occasion when we went to the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id      community                                               text  \\\n",
       "0      4341  poor-families  The scheme saw an estimated 150,000 children f...   \n",
       "1      4136       homeless  Durban 's homeless communities reconciliation ...   \n",
       "2     10352  poor-families  The next immediate problem that cropped up was...   \n",
       "3      8279     vulnerable  Far more important than the implications for t...   \n",
       "4      1164  poor-families  To strengthen child-sensitive social protectio...   \n",
       "...     ...            ...                                                ...   \n",
       "8370   8380        refugee  Rescue teams search for survivors on the rubbl...   \n",
       "8371   8381       hopeless  The launch of ' Happy Birthday ' took place la...   \n",
       "8372   8382       homeless  The unrest has left at least 20,000 people dea...   \n",
       "8373   8383       hopeless  You have to see it from my perspective . I may...   \n",
       "8374   8384       disabled  Yet there was one occasion when we went to the...   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "8370      0  \n",
       "8371      0  \n",
       "8372      0  \n",
       "8373      0  \n",
       "8374      0  \n",
       "\n",
       "[8375 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # will contain par_id, label and text\n",
    "for idx in range(len(teids)):\n",
    "    parid = teids.par_id[idx]\n",
    "    # print(parid)\n",
    "    # select row from original dataset\n",
    "    keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "    text = data.loc[data.par_id == parid].text.values[0]\n",
    "    label = data.loc[data.par_id == parid].label.values[0]\n",
    "    rows.append(\n",
    "        {\"par_id\": parid, \"community\": keyword, \"text\": text, \"label\": label}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedf1 = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/16 [25:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m task1_model \u001b[38;5;241m=\u001b[39m ClassificationModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m                                   args \u001b[38;5;241m=\u001b[39m task1_model_args, \n\u001b[1;32m      8\u001b[0m                                   num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m      9\u001b[0m                                   use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtask1_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# run predictions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m preds_task1, _ \u001b[38;5;241m=\u001b[39m task1_model\u001b[38;5;241m.\u001b[39mpredict(tedf1\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:617\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         )\n\u001b[1;32m    613\u001b[0m         train_examples \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    614\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    615\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    616\u001b[0m         )\n\u001b[0;32m--> 617\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    621\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    622\u001b[0m     train_dataset,\n\u001b[1;32m    623\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    624\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    625\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1824\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[38;5;241m+\u001b[39m chunksize], \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer, args\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(args\u001b[38;5;241m.\u001b[39mprocess_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_data_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     examples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[38;5;241m.\u001b[39mcat([example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/vol/bitbucket/ss5120/nlp_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task1_model_args = ClassificationArgs(num_train_epochs=1, \n",
    "                                      no_save=True, \n",
    "                                      no_cache=True, \n",
    "                                      overwrite_output_dir=True)\n",
    "task1_model = ClassificationModel(\"roberta\", \n",
    "                                  'roberta-base', \n",
    "                                  args = task1_model_args, \n",
    "                                  num_labels=2, \n",
    "                                  use_cuda=True)\n",
    "# train model\n",
    "task1_model.train_model(trdf1[['text', 'label']])\n",
    "# run predictions\n",
    "preds_task1, _ = task1_model.predict(tedf1.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
